{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 텍스트는 가장 흔한 시퀀스 형태의 데이터임.\n",
    "+ 텍스트 데이터를 처리하기 위해서는 **텍스트 벡터화**(vectoring text)를 거쳐야 함. 다음과 같은 방식을 고려할 수 있음.\n",
    "    + 텍스트를 단어로 나누고 각 단어를 하나의 벡터로 변환함.\n",
    "    + 텍스트를 문자로 나누고 각 문자를 하나의 벡터로 변환함.\n",
    "    + 텍스트에서 단어나 문자의 n-그램(n-gram)을 추출하여 각 n-그램을 하나의 벡터로 변환함ㅁ. n-그램은 단어나 문자의 그룹으로 텍스트에서 단어나 문자를 하나씩 이동하면서 추출함.텍스트를 나누는 이런 단위(단어, 문자, n-그램)를 **토큰**(token)이라 함. 그리고 텍스트를 토큰으로 나누는 작업을 **토큰화**(tokenization)라고 함.\n",
    "+ 모든 텍스트 벡터화 과정은 어떤 종류의 토큰화를 적용하고 생성된 토큰에 수치형 벡터를 연결하는 것으로 이뤄짐.\n",
    "+ 토큰과 벡터를 연결하는 방법은 여러 가지가 있음.\n",
    "    + 토큰의 **원 핫 인코딩**(one-hot encoding)\n",
    "    + **토큰 임베딩**(token emdedding) 혹은 **단어 임베딩**(word embedding)\n",
    "\n",
    "\n",
    "| 단위 | 예시|\n",
    "| :--- | :---: |\n",
    "| 텍스트 | \"The cat sat on the mat.\" | \n",
    "| 토큰 | \"The\", \"cat\", \"sat\",\"on\", \"the\", \"mat\", \".\"|{float:left}\n",
    "| 토큰의 벡터 인코딩| |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">6.1.1.</span> 단어와 문자의 원-핫 인코딩\n",
    "\n",
    "+ 원 핫 인코딩은 토큰을 벡터로 변환하는 가장 일반적이고 기본적인 방법.\n",
    "+ 모든 단어에 고유한 정수 인덱스를 부여하고 이 정수 인덱스 i를 크기가 N(어휘 사전의 크기)인 이진 벡터로 변환함.\n",
    "+ 이 벡터는 i번째 원소만 1이고 나머지는 모두 0임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 예시: The cat sat on the mat.\n",
      "\n",
      "토큰 인덱스: {'The': 1, 'cat': 2, 'sat': 3, 'on': 4, 'the': 5, 'mat.': 6, 'dog': 7, 'ate': 8, 'my': 9, 'homework.': 10}\n",
      "\n",
      "첫 문장 변환 결과:\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 단어 수준의 원-핫 인코딩 하기\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.'] # 초기 데이터: 각 원소가 샘플임.\n",
    "\n",
    "token_index = {} # 데이터에 있는 모든 토큰의 인덱스를 구축\n",
    "for sample in samples:\n",
    "    for word in sample.split(): # split() 메서드를 사용하여 샘플을 토큰으로 나눔. 실전에서는 구두점과 특수 문자도 사용함.\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) +1 # 단어마다 고유한 인덱스 할당. 관례상 인덱스 0은 사용하지 않음.\n",
    "\n",
    "max_length = 10 # 샘플을 벡터로 변환함. 각 샘플에서 max_length까지 단어만 사용함.\n",
    "\n",
    "results = np.zeros(shape = (len(samples),\n",
    "                   max_length,\n",
    "                   max(token_index.values())+1)) # 결과를 저장할 배열\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1.\n",
    "        \n",
    "# 결과 예시: 문장: The cat sat on the mat.\n",
    "print(\"문장 예시: {0}\\n\\n토큰 인덱스: {1}\\n\\n첫 문장 변환 결과:\\n{2}\".format(samples[0],token_index, results[0,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 예시: The cat sat on the mat.\n",
      "\n",
      "문자 인덱스:\n",
      "{'0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'a': 11, 'b': 12, 'c': 13, 'd': 14, 'e': 15, 'f': 16, 'g': 17, 'h': 18, 'i': 19, 'j': 20, 'k': 21, 'l': 22, 'm': 23, 'n': 24, 'o': 25, 'p': 26, 'q': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'x': 34, 'y': 35, 'z': 36, 'A': 37, 'B': 38, 'C': 39, 'D': 40, 'E': 41, 'F': 42, 'G': 43, 'H': 44, 'I': 45, 'J': 46, 'K': 47, 'L': 48, 'M': 49, 'N': 50, 'O': 51, 'P': 52, 'Q': 53, 'R': 54, 'S': 55, 'T': 56, 'U': 57, 'V': 58, 'W': 59, 'X': 60, 'Y': 61, 'Z': 62, '!': 63, '\"': 64, '#': 65, '$': 66, '%': 67, '&': 68, \"'\": 69, '(': 70, ')': 71, '*': 72, '+': 73, ',': 74, '-': 75, '.': 76, '/': 77, ':': 78, ';': 79, '<': 80, '=': 81, '>': 82, '?': 83, '@': 84, '[': 85, '\\\\': 86, ']': 87, '^': 88, '_': 89, '`': 90, '{': 91, '|': 92, '}': 93, '~': 94, ' ': 95, '\\t': 96, '\\n': 97, '\\r': 98, '\\x0b': 99, '\\x0c': 100}\n",
      "\n",
      "첫 문장 변환 결과:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 문자 수준의 원-핫 인코딩 하기\n",
    "\n",
    "import string\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "characters = string.printable # 출력가능한 모든 아스키(ASCII) 문자\n",
    "token_index = dict(zip(characters, range(1, len(characters) +1)))\n",
    "\n",
    "max_length = 50\n",
    "results = np.zeros((len(samples), max_length, max(token_index.values())+1))\n",
    "\n",
    "for i, sample in enumerate(samples): \n",
    "    for j, character in enumerate(sample):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1.\n",
    "        \n",
    "# 결과 예시\n",
    "print(\"문장 예시: {0}\\n\\n문자 인덱스:\\n{1}\\n\\n첫 문장 변환 결과:\\n{2}\".format(samples[0],token_index, results[0,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9개의 고유한 토큰을 찾았습니다.\n",
      "\n",
      "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
     ]
    }
   ],
   "source": [
    "# 케라스를 사용한 단어 수준의 원-핫 인코딩하기\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000) # 가장 빈도가 높은 1000개의 단어만 선택하도록 Tokenizer 객체 생성\n",
    "tokenizer.fit_on_texts(samples) # 단어 인덱스 구축\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples) # 문자열을 정수 인덱스의 리스트로 변환\n",
    "\n",
    "# 직접 원-핫 이진 벡터 표현을 획득하기.\n",
    "ont_hot_results = tokenizer.texts_to_matrix(samples, mode = 'binary') # 다른 벡터화 방법도 있음.\n",
    "\n",
    "word_index = tokenizer.word_index # 계산된 단어 인덱스 계산\n",
    "print('%s개의 고유한 토큰을 찾았습니다.\\n' % len(word_index))\n",
    "\n",
    "# word index 출력\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\">Note.</span> 단어와 문자의 원-핫 해싱\n",
    "\n",
    "+ 원-핫 인코딩의 변종 중 하나는 **원-핫 해싱**(ont-hot hashing) 기법임.\n",
    "+ 이 방법은 어휘 사전에 있는 고유한 토큰의 수가 너무 많아서 모두 다루기 어려울 때 사용함. 단어를 해싱하여 고정된 크기의 벡터로 변환함. \n",
    "+ 이 방법의 장점은 명시적인 단어 인덱스가 필요없기 때문에 메모리를 절약하고 온라인 방식으로 데이터를 인코딩할 수 있다는 것임. 즉 전체 데이터를 확인하지 않고 토큰을 생성할 수 있음.\n",
    "+ 한 가지 단점은 **해시 충돌**(hash collision)임. 두 개의 단어가 같은 해시를 만들면 머신 러닝 모델은 그 둘 간의 차이를 인지하지 못함.\n",
    "+ 해싱 공간의 차원이 해싱될 고유 토큰의 전체 개수보다 훨씬 그면 해시 충돌의 가능성은 감소함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "989\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 해싱 기법을 사용한 단어 수준의 원-핫 인코딩 하기\n",
    "\n",
    "samples = ['The cat sat on the mat']\n",
    "\n",
    "# 단어를 크기가 1000인 벡터로 저장함. 1000개 또는 그 이상의 단어가 있다면 해싱 충돌이 늘어다고 인코딩의 정확도가 감소될 것임.\n",
    "dimensionality = 1000 \n",
    "max_lengt = 10\n",
    "\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = abs(hash(word)) % dimensionality # 단어를 해싱하여 0과 1000사이의 랜덤한 정수 인덱스로 변환함.\n",
    "        results[i, j, index] = 1.\n",
    "\n",
    "# 결과 출력\n",
    "print(index)\n",
    "print(results[0, :,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">6.1.2.</span> 단어 임베딩 사용하기\n",
    "\n",
    "+ 단어와 벡터를 연관짓는 강력하고 인기있는 또 다른 방법은 **단어 임베딩**이라는 밀집 **단어 벡터**(word vector)를 사용하는 것임.\n",
    "+ 원-핫 인코딩으로 만든 벡터는 희소(sparse)하고(대부분 0으로 채워짐) 고차원임(어휘 사전에 있는 단어의 수와 차원이 같음).\n",
    "+ 반면 단어 임베딩은 저차원의 실수형 벡터임(희소벡터 ↔ 밀집 벡터).\n",
    "+ 단어 임베딩은 데이터로부터 학습됨. 보통 256차원, 512 차원 또는 큰 어휘 사전을 다룰 때는 1024차원의 단어 임베딩을 사용함.\n",
    "+ 반면, 원-핫 인코딩은 20000차원 또는 그 이상의 벡터인 경우가 많음. 따라서 단어 임베딩이 더 많은 정보를 더 적은 차원에 저장함.\n",
    "\n",
    "*원-핫 코딩과 단어 임베딩 비교*\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/sgNTmwg/539-DEDE0-306-F-4132-B1-BC-3-A6-ACDF9-C679-png.jpg\" alt=\"539-DEDE0-306-F-4132-B1-BC-3-A6-ACDF9-C679-png\" border=\"0\" align='left'></a><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 단어 임베딩을 만드는 방법은 두 가지가 있음.\n",
    "    + (문서 분류나 감성 예측 같은) 관심 대상인 문제와 함께 단어 임베딩을 학습함. 이런 경우 랜덤한 단어 벡터로 시작해서 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습함.\n",
    "    + 풀려는 문제가 아니고 다른 머신 러닝 작업에서 미리 계산된 단어 임베딩을 로드함. 이를 **사전 훈련된 임베딩**(pretrained word embedding)이라고 함.\n",
    "    \n",
    "### Emdedding 층을 사용하여 단어임베딩 학습하기\n",
    "+ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
